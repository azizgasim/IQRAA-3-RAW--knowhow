# الجزء الثاني: الأكواد والتفاصيل العملية
## Infrastructure Agent: Implementation Guide

---

# 11. فلسفة التكرار العالي القيمة

## 11.1 المشكلة المعرفية الفريدة

```
┌─────────────────────────────────────────────────────────────┐
│           مثال: شروح المنهاج للنووي                         │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  المنهاج (المتن الأصلي) ───────────────────── 100 صفحة     │
│       │                                                     │
│       ├── مغني المحتاج (الشربيني) ────────── 2000 صفحة     │
│       │   └── 85% = نص المتن + شرح مباشر                   │
│       │   └── 15% = اجتهادات الشربيني الفريدة              │
│       │                                                     │
│       ├── نهاية المحتاج (الرملي) ─────────── 2500 صفحة     │
│       │   └── 80% = متشابه مع مغني المحتاج                 │
│       │   └── 20% = اختيارات الرملي المختلفة               │
│       │                                                     │
│       ├── تحفة المحتاج (ابن حجر) ─────────── 3000 صفحة     │
│       │   └── 75% = متشابه                                 │
│       │   └── 25% = منهج ابن حجر المميز                    │
│       │                                                     │
│       └── حاشية الشرواني ──────────────────── 1500 صفحة    │
│           └── 90% = نقل من السابقين                        │
│           └── 10% = إضافات وتحقيقات                        │
│                                                             │
│  المجموع الخام: ~9000 صفحة                                  │
│  المحتوى الفريد فعلياً: ~2000 صفحة                         │
│  التكلفة بدون تحسين: 4.5x الضرورة!                         │
│                                                             │
│  لكن الـ 15-25% الفريدة = كنز معرفي:                       │
│  • تكشف تطور المذهب عبر القرون                             │
│  • تبين الخلاف داخل المذهب الواحد                          │
│  • توثق السياق التاريخي والجغرافي                          │
│  • تحفظ اجتهادات لا توجد في مكان آخر                       │
└─────────────────────────────────────────────────────────────┘
```

## 11.2 الحل: معمارية الأصل والدلتا (Origin-Delta Architecture)

```sql
-- ═══════════════════════════════════════════════════════════
-- الجدول 1: الأعمال الأصلية (Core Works)
-- ═══════════════════════════════════════════════════════════
CREATE TABLE `iqra12.curated.core_works` (
  work_id STRING NOT NULL,
  title STRING NOT NULL,
  title_normalized STRING,  -- للبحث
  author_id STRING,
  work_type ENUM('matn', 'sharh', 'hashiya', 'mukhtasar', 'nazm') NOT NULL,
  parent_work_id STRING,    -- للشرح: ما المتن؟
  language STRING DEFAULT 'ar',
  century_composed INT64,
  word_count INT64,
  is_base_text BOOL,        -- هل هذا متن أصلي أم مشتق؟
  
  -- الميتاداتا الأكاديمية
  scholarly_importance ENUM('foundational', 'major', 'standard', 'minor'),
  madhab STRING,
  field STRING,             -- فقه، عقيدة، تفسير...
  
  -- التتبع
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
  updated_at TIMESTAMP,
  corpus_version STRING
)
PARTITION BY RANGE_BUCKET(century_composed, GENERATE_ARRAY(1, 15, 1))
CLUSTER BY madhab, work_type, field;

-- ═══════════════════════════════════════════════════════════
-- الجدول 2: المقاطع النصية (Passages)
-- ═══════════════════════════════════════════════════════════
CREATE TABLE `iqra12.curated.passages` (
  passage_id STRING NOT NULL,
  work_id STRING NOT NULL,
  
  -- الموقع
  volume INT64,
  page_start INT64,
  page_end INT64,
  chapter_id STRING,
  section_hierarchy ARRAY<STRING>,  -- ['كتاب الطهارة', 'باب الوضوء', 'فصل فرائضه']
  
  -- النص
  text STRING NOT NULL,
  text_normalized STRING,           -- بدون تشكيل للبحث
  word_count INT64,
  
  -- التصنيف المعرفي
  content_type ENUM('matn_quote', 'sharh_direct', 'tawjih', 'dalil', 'furu', 'khilaf', 'tarjih'),
  
  -- علامات الجودة
  quality_score FLOAT64,            -- 0-1
  ocr_confidence FLOAT64,
  has_issues BOOL DEFAULT FALSE,
  issue_types ARRAY<STRING>,
  
  -- الربط
  quotes_passage_id STRING,         -- إذا كان يقتبس مقطعاً آخر
  embedding ARRAY<FLOAT64>,         -- Vector للبحث الدلالي
  
  -- التتبع
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
  corpus_version STRING
)
PARTITION BY RANGE_BUCKET(
  CAST(SUBSTR(work_id, 2) AS INT64), 
  GENERATE_ARRAY(0, 10000, 100)
)
CLUSTER BY work_id, chapter_id, content_type;

-- ═══════════════════════════════════════════════════════════
-- الجدول 3: علاقات التناص (Intertextuality)
-- ═══════════════════════════════════════════════════════════
CREATE TABLE `iqra12.semantic.intertextuality` (
  relation_id STRING NOT NULL,
  
  -- الطرفان
  source_passage_id STRING NOT NULL,      -- الأصل
  derived_passage_id STRING NOT NULL,     -- المشتق
  
  -- طبيعة العلاقة
  relation_type ENUM(
    'verbatim',           -- نقل حرفي
    'paraphrase',         -- إعادة صياغة
    'summary',            -- تلخيص
    'expansion',          -- توسع وشرح
    'commentary',         -- تعليق
    'disagreement',       -- خلاف
    'refinement'          -- تنقيح وتحرير
  ) NOT NULL,
  
  -- القياسات
  similarity_score FLOAT64,       -- 0-1 تشابه لفظي
  semantic_similarity FLOAT64,    -- 0-1 تشابه معنوي
  delta_significance ENUM('trivial', 'minor', 'notable', 'major', 'critical'),
  
  -- الدلتا (الفرق)
  delta_summary STRING,           -- وصف مختصر للفرق
  delta_type ARRAY<STRING>,       -- ['إضافة دليل', 'ترجيح', 'تفريع']
  
  -- الثقة
  detection_method ENUM('algorithmic', 'manual', 'hybrid'),
  confidence FLOAT64,
  verified_by STRING,
  
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
)
CLUSTER BY source_passage_id, relation_type;

-- ═══════════════════════════════════════════════════════════
-- الجدول 4: الدلتا العالية القيمة (High-Value Deltas)
-- ═══════════════════════════════════════════════════════════
CREATE TABLE `iqra12.semantic.valuable_deltas` (
  delta_id STRING NOT NULL,
  relation_id STRING NOT NULL,    -- من جدول التناص
  
  -- المحتوى
  source_text STRING,             -- النص الأصلي
  derived_text STRING,            -- النص المشتق
  delta_content STRING,           -- الفرق المستخلص
  
  -- التصنيف المعرفي
  delta_category ENUM(
    'ijtihad',                    -- اجتهاد جديد
    'tarjih',                     -- ترجيح بين أقوال
    'tafri',                      -- تفريع فقهي
    'dalil_jadid',                -- دليل جديد
    'radd',                       -- رد على قول
    'tawfiq',                     -- توفيق بين أقوال
    'taqyid',                     -- تقييد مطلق
    'takhsis',                    -- تخصيص عام
    'context_note'                -- ملاحظة سياقية
  ),
  
  -- القيمة
  scholarly_value ENUM('low', 'medium', 'high', 'exceptional'),
  value_justification STRING,
  
  -- الربط
  related_concepts ARRAY<STRING>,
  related_scholars ARRAY<STRING>,
  related_debates ARRAY<STRING>,
  
  -- الفهرسة
  keywords ARRAY<STRING>,
  embedding ARRAY<FLOAT64>,
  
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
)
CLUSTER BY delta_category, scholarly_value;
```

## 11.3 خوارزمية كشف التكرار والقيمة

```python
"""
خوارزمية كشف التناص واستخلاص الدلتا
Intertextuality Detection & Delta Extraction
"""

from dataclasses import dataclass
from typing import List, Tuple, Optional
from enum import Enum
import numpy as np

class RelationType(Enum):
    VERBATIM = 'verbatim'
    PARAPHRASE = 'paraphrase'
    SUMMARY = 'summary'
    EXPANSION = 'expansion'
    COMMENTARY = 'commentary'
    DISAGREEMENT = 'disagreement'
    REFINEMENT = 'refinement'

class DeltaSignificance(Enum):
    TRIVIAL = 'trivial'      # اختلاف إملائي/تشكيلي
    MINOR = 'minor'          # إعادة صياغة بسيطة
    NOTABLE = 'notable'      # إضافة توضيحية
    MAJOR = 'major'          # اجتهاد أو ترجيح
    CRITICAL = 'critical'    # خلاف جوهري أو قول جديد

@dataclass
class IntertextualityResult:
    source_id: str
    derived_id: str
    relation_type: RelationType
    similarity_score: float
    semantic_similarity: float
    delta_significance: DeltaSignificance
    delta_content: Optional[str]
    delta_summary: Optional[str]

class IntertextualityDetector:
    """
    كاشف التناص ومستخلص الدلتا
    
    المبدأ: لا نخزن المكرر، نخزن الفرق ونربطه بالأصل
    """
    
    def __init__(self, embedding_model, bq_client):
        self.embedder = embedding_model
        self.bq = bq_client
        
        # عتبات التصنيف
        self.VERBATIM_THRESHOLD = 0.95      # 95%+ = نقل حرفي
        self.PARAPHRASE_THRESHOLD = 0.85    # 85-95% = إعادة صياغة
        self.RELATED_THRESHOLD = 0.70       # 70-85% = محتوى مرتبط
        
    def detect_intertextuality(
        self, 
        source_passage: str, 
        candidate_passages: List[Tuple[str, str]]  # [(id, text), ...]
    ) -> List[IntertextualityResult]:
        """
        كشف علاقات التناص بين مقطع ومجموعة مقاطع مرشحة
        """
        results = []
        
        # حساب embedding للمصدر
        source_emb = self.embedder.encode(source_passage)
        
        for cand_id, cand_text in candidate_passages:
            # 1. التشابه اللفظي (Jaccard على n-grams)
            lexical_sim = self._lexical_similarity(source_passage, cand_text)
            
            # 2. التشابه الدلالي (Cosine على embeddings)
            cand_emb = self.embedder.encode(cand_text)
            semantic_sim = self._cosine_similarity(source_emb, cand_emb)
            
            # 3. تصنيف العلاقة
            relation_type = self._classify_relation(lexical_sim, semantic_sim)
            
            # 4. استخلاص الدلتا
            delta_content, delta_summary = self._extract_delta(
                source_passage, cand_text, relation_type
            )
            
            # 5. تقييم أهمية الدلتا
            delta_sig = self._assess_delta_significance(
                delta_content, relation_type
            )
            
            results.append(IntertextualityResult(
                source_id=source_passage[:20],  # placeholder
                derived_id=cand_id,
                relation_type=relation_type,
                similarity_score=lexical_sim,
                semantic_similarity=semantic_sim,
                delta_significance=delta_sig,
                delta_content=delta_content,
                delta_summary=delta_summary
            ))
        
        return results
    
    def _lexical_similarity(self, text1: str, text2: str) -> float:
        """حساب التشابه اللفظي باستخدام n-grams"""
        def get_ngrams(text, n=3):
            words = text.split()
            return set(' '.join(words[i:i+n]) for i in range(len(words)-n+1))
        
        ngrams1 = get_ngrams(text1)
        ngrams2 = get_ngrams(text2)
        
        if not ngrams1 or not ngrams2:
            return 0.0
        
        intersection = len(ngrams1 & ngrams2)
        union = len(ngrams1 | ngrams2)
        
        return intersection / union if union > 0 else 0.0
    
    def _cosine_similarity(self, emb1: np.ndarray, emb2: np.ndarray) -> float:
        """حساب التشابه الدلالي"""
        return float(np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2)))
    
    def _classify_relation(self, lexical: float, semantic: float) -> RelationType:
        """تصنيف نوع العلاقة"""
        if lexical >= self.VERBATIM_THRESHOLD:
            return RelationType.VERBATIM
        elif lexical >= self.PARAPHRASE_THRESHOLD:
            return RelationType.PARAPHRASE
        elif semantic >= 0.9 and lexical < 0.7:
            return RelationType.SUMMARY  # معنى متطابق، لفظ مختلف
        elif semantic >= 0.85:
            if lexical < semantic - 0.2:
                return RelationType.EXPANSION
            else:
                return RelationType.REFINEMENT
        elif semantic >= self.RELATED_THRESHOLD:
            return RelationType.COMMENTARY
        else:
            return RelationType.DISAGREEMENT
    
    def _extract_delta(
        self, source: str, derived: str, relation_type: RelationType
    ) -> Tuple[Optional[str], Optional[str]]:
        """استخلاص الفرق بين النصين"""
        if relation_type == RelationType.VERBATIM:
            return None, None  # لا فرق
        
        # استخدام diff بسيط (في الإنتاج: LLM للتلخيص)
        source_words = set(source.split())
        derived_words = set(derived.split())
        
        added = derived_words - source_words
        removed = source_words - derived_words
        
        delta_content = None
        if added:
            # البحث عن الجمل الجديدة
            delta_content = self._find_new_sentences(source, derived)
        
        delta_summary = f"أضاف {len(added)} كلمة، حذف {len(removed)} كلمة"
        
        return delta_content, delta_summary
    
    def _find_new_sentences(self, source: str, derived: str) -> str:
        """البحث عن الجمل الموجودة في المشتق وليست في الأصل"""
        source_sentences = set(source.split('.'))
        derived_sentences = derived.split('.')
        
        new_sentences = []
        for sent in derived_sentences:
            if sent.strip() and sent not in source_sentences:
                # تحقق إضافي: هل الجملة جديدة فعلاً؟
                is_new = all(
                    self._lexical_similarity(sent, src) < 0.7 
                    for src in source_sentences if src.strip()
                )
                if is_new:
                    new_sentences.append(sent.strip())
        
        return ' | '.join(new_sentences[:5])  # أول 5 جمل جديدة
    
    def _assess_delta_significance(
        self, delta_content: Optional[str], relation_type: RelationType
    ) -> DeltaSignificance:
        """تقييم أهمية الدلتا"""
        if delta_content is None:
            return DeltaSignificance.TRIVIAL
        
        # كلمات دالة على الأهمية
        MAJOR_INDICATORS = [
            'والراجح', 'والصحيح', 'والمعتمد', 'خلافاً', 'ردّ', 
            'اعترض', 'الأصح', 'المختار', 'فائدة', 'تنبيه'
        ]
        
        CRITICAL_INDICATORS = [
            'وهذا قول جديد', 'لم أر من ذكره', 'وهو مشكل',
            'والجواب', 'إشكال', 'وفيه نظر'
        ]
        
        delta_lower = delta_content.lower() if delta_content else ''
        
        if any(ind in delta_content for ind in CRITICAL_INDICATORS):
            return DeltaSignificance.CRITICAL
        elif any(ind in delta_content for ind in MAJOR_INDICATORS):
            return DeltaSignificance.MAJOR
        elif relation_type in [RelationType.DISAGREEMENT, RelationType.EXPANSION]:
            return DeltaSignificance.NOTABLE
        elif relation_type == RelationType.COMMENTARY:
            return DeltaSignificance.MINOR
        else:
            return DeltaSignificance.TRIVIAL


# ═══════════════════════════════════════════════════════════
# الاستخدام في Pipeline
# ═══════════════════════════════════════════════════════════

def process_new_work(work_id: str, passages: List[dict], detector: IntertextualityDetector):
    """
    معالجة كتاب جديد: كشف التكرار وتخزين الدلتا فقط
    """
    results = {
        'verbatim_count': 0,
        'stored_deltas': 0,
        'storage_saved_mb': 0
    }
    
    for passage in passages:
        # البحث عن مقاطع مشابهة في القاعدة
        similar = detector.bq.query(f"""
            SELECT passage_id, text
            FROM `iqra12.curated.passages`
            WHERE COSINE_DISTANCE(embedding, @query_embedding) < 0.3
            LIMIT 10
        """, query_embedding=passage['embedding'])
        
        if similar:
            # كشف العلاقات
            relations = detector.detect_intertextuality(
                passage['text'], 
                [(r['passage_id'], r['text']) for r in similar]
            )
            
            for rel in relations:
                if rel.relation_type == RelationType.VERBATIM:
                    # لا نخزن! نربط فقط
                    results['verbatim_count'] += 1
                    results['storage_saved_mb'] += len(passage['text']) / 1_000_000
                    
                    # تخزين الربط فقط
                    store_reference(passage['id'], rel.source_id)
                    
                elif rel.delta_significance.value in ['major', 'critical']:
                    # نخزن الدلتا العالية القيمة
                    results['stored_deltas'] += 1
                    store_valuable_delta(rel)
        else:
            # مقطع جديد تماماً - نخزنه كاملاً
            store_full_passage(passage)
    
    return results
```

---

# 12. أفضل ممارسات تصميم الجداول

## 12.1 قواعد العدد والتنظيم

```
┌─────────────────────────────────────────────────────────────┐
│              قواعد تصميم الجداول في إقرأ-12                 │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  القاعدة 1: جدول لكل كيان مستقل                            │
│  ─────────────────────────────────────────────────────────  │
│  ✓ scholars (علماء)                                        │
│  ✓ works (أعمال)                                           │
│  ✓ passages (مقاطع)                                        │
│  ✗ scholars_and_works (دمج = كارثة)                        │
│                                                             │
│  القاعدة 2: جدول لكل نوع علاقة معقدة                       │
│  ─────────────────────────────────────────────────────────  │
│  ✓ teacher_student (إسناد)                                 │
│  ✓ intertextuality (تناص)                                  │
│  ✓ concept_hierarchy (تسلسل مفاهيمي)                       │
│                                                             │
│  القاعدة 3: Dataset لكل طبقة                               │
│  ─────────────────────────────────────────────────────────  │
│  iqra12_raw        → 3-5 جداول                             │
│  iqra12_curated    → 8-12 جدول                             │
│  iqra12_semantic   → 10-15 جدول                            │
│  iqra12_evidence   → 5-8 جداول                             │
│  iqra12_operations → 6-10 جداول                            │
│                                                             │
│  القاعدة 4: تجنب الانفجار الجدولي                          │
│  ─────────────────────────────────────────────────────────  │
│  ✗ جدول لكل كتاب (1000 جدول!)                             │
│  ✓ جدول واحد + partition بالكتاب                          │
│  ✗ جدول لكل سنة (100 جدول!)                               │
│  ✓ جدول واحد + partition بالعقد/القرن                     │
│                                                             │
│  العدد المثالي الإجمالي: 40-60 جدول                        │
│  (أقل = ضغط غير صحي، أكثر = تعقيد غير مبرر)                │
└─────────────────────────────────────────────────────────────┘
```

## 12.2 موجهات اختيار التصميم

```
┌─────────────────────────────────────────────────────────────┐
│           شجرة قرارات تصميم الجدول                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  السؤال 1: هل البيانات كبيرة (>1GB)؟                       │
│  │                                                          │
│  ├── نعم ──► السؤال 2: هل هناك حقل زمني؟                   │
│  │           │                                              │
│  │           ├── نعم ──► Partition بالتاريخ                 │
│  │           │           (ingestion_date, death_century)    │
│  │           │                                              │
│  │           └── لا ───► السؤال 3: هل هناك حقل توزيعي؟     │
│  │                       │                                  │
│  │                       ├── نعم ──► Partition بالنطاق      │
│  │                       │           (RANGE_BUCKET)         │
│  │                       │                                  │
│  │                       └── لا ───► Clustering فقط        │
│  │                                                          │
│  └── لا ───► لا partition، ربما clustering                 │
│                                                             │
│  السؤال 4: ما الاستعلامات الأكثر تكراراً؟                  │
│  │                                                          │
│  ├── فلترة بحقل واحد ──► Cluster على هذا الحقل            │
│  │                                                          │
│  ├── فلترة بحقلين ────► Cluster على الحقلين بالترتيب       │
│  │                       (الأكثر تصفية أولاً)               │
│  │                                                          │
│  └── بحث نصي ─────────► + Vector Index للـ embeddings       │
│                                                             │
│  السؤال 5: هل هناك علاقات 1:N كثيفة؟                       │
│  │                                                          │
│  ├── نعم + الـ N صغير (<100) ──► ARRAY/STRUCT داخل الجدول │
│  │                                                          │
│  └── نعم + الـ N كبير ─────────► جدول منفصل + JOIN         │
└─────────────────────────────────────────────────────────────┘
```

## 12.3 جدول القرارات التفصيلي

| السيناريو | الحل الأمثل | السبب الأكاديمي | السبب التقني | السبب الاقتصادي |
|-----------|-------------|-----------------|--------------|-----------------|
| علماء المذهب | Partition by century, Cluster by madhab | فترات تاريخية مميزة | استعلامات بالقرن شائعة | تقليل المسح 90% |
| نصوص الكتب | Partition by work_id ranges | كل كتاب وحدة منطقية | استعلامات بالكتاب | تجنب مسح كل النصوص |
| علاقات التناص | Cluster by source_id, relation_type | التتبع من الأصل | البحث عن مشتقات | تسريع JOINs |
| Embeddings | Separate table + Vector Index | البحث الدلالي مستقل | VECTOR_SEARCH محسّن | تكلفة البحث فقط |
| سجلات التشغيل | Partition by day, Cluster by agent_id | تحليل يومي | حذف القديم سهل | تخزين طويل المدى رخيص |

---

# 13. ميكانيكية عمل BigQuery: الصديق والخطر

## 13.1 كيف يعمل الاستعلام؟

```
┌─────────────────────────────────────────────────────────────┐
│              رحلة الاستعلام في BigQuery                      │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  1. الاستقبال                                               │
│     ┌─────────────────────────────────────────────────┐    │
│     │ SELECT author, COUNT(*)                          │    │
│     │ FROM passages                                    │    │
│     │ WHERE madhab = 'Shafi' AND century = 7           │    │
│     │ GROUP BY author                                  │    │
│     └─────────────────────────────────────────────────┘    │
│                          │                                  │
│                          ▼                                  │
│  2. التحليل (Query Planning)                               │
│     • تحديد الجداول المطلوبة                               │
│     • تحديد الأعمدة المقروءة (author فقط!)                 │
│     • تحديد الأقسام المطلوبة (century=7)                   │
│     • تحديد الـ clusters المطلوبة (madhab='Shafi')         │
│                          │                                  │
│                          ▼                                  │
│  3. التقدير (Dry Run)                                      │
│     ┌─────────────────────────────────────────────────┐    │
│     │ "This query will process 2.3 GB"                 │    │
│     │ "Estimated cost: $0.0115"                        │    │
│     └─────────────────────────────────────────────────┘    │
│                          │                                  │
│                          ▼                                  │
│  4. التوزيع (Distribution)                                 │
│     ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐ ┌─────┐              │
│     │Slot1│ │Slot2│ │Slot3│ │Slot4│ │...  │              │
│     └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘ └──┬──┘              │
│        │       │       │       │       │                   │
│        ▼       ▼       ▼       ▼       ▼                   │
│     [قراءة] [قراءة] [قراءة] [قراءة] [قراءة]              │
│     [فلترة] [فلترة] [فلترة] [فلترة] [فلترة]              │
│     [تجميع] [تجميع] [تجميع] [تجميع] [تجميع]              │
│        │       │       │       │       │                   │
│        └───────┴───────┴───────┴───────┘                   │
│                          │                                  │
│                          ▼                                  │
│  5. الدمج (Aggregation)                                    │
│     ┌─────────────────────────────────────────────────┐    │
│     │ author    │ count                                │    │
│     │ النووي    │ 1,234                                │    │
│     │ الرافعي   │ 987                                  │    │
│     │ الشيرازي  │ 654                                  │    │
│     └─────────────────────────────────────────────────┘    │
│                                                             │
│  الوقت الإجمالي: 2.3 ثانية (!)                             │
└─────────────────────────────────────────────────────────────┘
```

## 13.2 الصديق في الميكانيكية

```
┌─────────────────────────────────────────────────────────────┐
│                   العناصر الصديقة                           │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  1. Columnar Storage ✓                                      │
│     ─────────────────                                       │
│     • لا تقرأ إلا الأعمدة المطلوبة                         │
│     • الضغط فعّال جداً (نفس القيم متجاورة)                 │
│     • مثال: جدول 100 عمود، تحتاج 3 = تقرأ 3% فقط          │
│                                                             │
│  2. Automatic Optimization ✓                                │
│     ───────────────────────                                 │
│     • إعادة ترتيب JOINs تلقائياً                           │
│     • تبسيط الشروط                                         │
│     • حذف الأعمدة غير المستخدمة                            │
│                                                             │
│  3. Caching ✓                                               │
│     ─────────                                               │
│     • نتائج الاستعلام تُحفظ 24 ساعة                        │
│     • نفس الاستعلام = مجاني!                               │
│     • حتى لو اختلف المستخدم                                │
│                                                             │
│  4. Slots Elasticity ✓                                      │
│     ──────────────────                                      │
│     • لا تدفع للخوادم الفارغة                              │
│     • تتوسع حسب الحاجة (ثوانٍ)                             │
│     • تتقلص عند الفراغ                                     │
│                                                             │
│  5. Separation of Storage & Compute ✓                       │
│     ────────────────────────────────                        │
│     • خزّن بيتابايتات رخيصاً                               │
│     • استخدم قوة هائلة لثوانٍ                              │
│     • لا ترتبط بحجم التخزين                                │
└─────────────────────────────────────────────────────────────┘
```

## 13.3 الخطر في الميكانيكية

```
┌─────────────────────────────────────────────────────────────┐
│                   العناصر الخطرة                            │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  1. SELECT * ⚠️                                             │
│     ──────────                                              │
│     • تقرأ كل الأعمدة = تكلفة كارثية                       │
│     • مثال: جدول 10GB، تحتاج عمود 100MB                    │
│       - SELECT *     = $0.05 (10GB)                        │
│       - SELECT col   = $0.0005 (100MB)                     │
│       - الفرق: 100x!                                       │
│                                                             │
│  2. No Partition Filter ⚠️                                  │
│     ──────────────────────                                  │
│     • بدون WHERE على partition = مسح كامل                  │
│     • مثال: 10 سنوات بيانات، تريد يوم واحد                 │
│       - بدون فلتر = تقرأ 10 سنوات                         │
│       - مع فلتر  = تقرأ يوم واحد (0.03%)                  │
│                                                             │
│  3. Cross-Join Explosion ⚠️                                 │
│     ───────────────────────                                 │
│     • JOIN بدون شرط = ضرب ديكارتي                          │
│     • 1M × 1M = 1 Trillion صف!                             │
│     • الفاتورة: آلاف الدولارات                             │
│                                                             │
│  4. Repeated Expensive Queries ⚠️                           │
│     ─────────────────────────────                           │
│     • نفس الاستعلام المكلف يومياً                          │
│     • الحل: Materialized View أو Scheduled Query            │
│                                                             │
│  5. Streaming Inserts Abuse ⚠️                              │
│     ──────────────────────────                              │
│     • $0.01 لكل 200MB                                      │
│     • 1000 insert صغير = أغلى من batch واحد               │
│     • الحل: تجميع في buffer ثم batch load                  │
│                                                             │
│  6. ML Functions on Full Tables ⚠️                          │
│     ──────────────────────────────                          │
│     • ML.GENERATE_EMBEDDING على مليون صف                   │
│     • = مليون استدعاء API                                  │
│     • = فاتورة ضخمة                                        │
└─────────────────────────────────────────────────────────────┘
```

---

# 14. اقتصاديات البحث: هل نبحث في كل شيء؟

## 14.1 استراتيجية البحث الذكي

```
┌─────────────────────────────────────────────────────────────┐
│            لا تبحث في كل شيء - ابحث بذكاء                   │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  المستوى 1: البحث في الفهارس فقط                           │
│  ──────────────────────────────────                         │
│  • Vector Index للبحث الدلالي                              │
│  • الكلفة: ~0.001$ لكل 1000 بحث                           │
│  • يُرجع: passage_ids فقط                                  │
│                                                             │
│  المستوى 2: جلب المقاطع المطابقة                           │
│  ──────────────────────────────────                         │
│  • SELECT محدد على الـ IDs المسترجعة                       │
│  • الكلفة: حسب حجم المقاطع                                 │
│  • يُرجع: النصوص الكاملة                                   │
│                                                             │
│  المستوى 3: التوسع للسياق (عند الحاجة)                     │
│  ────────────────────────────────────────                   │
│  • جلب المقاطع المحيطة                                     │
│  • الكلفة: أعلى                                            │
│  • يُستخدم: للقراءة المعمقة فقط                            │
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │     الاستعلام                                       │   │
│  │         │                                           │   │
│  │         ▼                                           │   │
│  │    ┌─────────┐                                      │   │
│  │    │ Vector  │ ◄── رخيص جداً                       │   │
│  │    │ Index   │                                      │   │
│  │    └────┬────┘                                      │   │
│  │         │ top_k IDs                                 │   │
│  │         ▼                                           │   │
│  │    ┌─────────┐                                      │   │
│  │    │ Passages│ ◄── متوسط                           │   │
│  │    │ Table   │                                      │   │
│  │    └────┬────┘                                      │   │
│  │         │ (if needed)                               │   │
│  │         ▼                                           │   │
│  │    ┌─────────┐                                      │   │
│  │    │ Context │ ◄── أعلى (عند الطلب)                │   │
│  │    │ Expand  │                                      │   │
│  │    └─────────┘                                      │   │
│  └─────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
```

## 14.2 SQL للبحث الاقتصادي

```sql
-- ═══════════════════════════════════════════════════════════
-- البحث الذكي: 3 مراحل
-- ═══════════════════════════════════════════════════════════

-- المرحلة 1: البحث في Vector Index (الأرخص)
CREATE OR REPLACE PROCEDURE `iqra12.search.semantic_search`(
  query_text STRING,
  top_k INT64,
  OUT results ARRAY<STRUCT<passage_id STRING, score FLOAT64>>
)
BEGIN
  -- توليد embedding للاستعلام
  DECLARE query_embedding ARRAY<FLOAT64>;
  
  SET query_embedding = (
    SELECT ml_generate_embedding_result
    FROM ML.GENERATE_EMBEDDING(
      MODEL `iqra12.models.text_embedding`,
      (SELECT query_text AS content)
    )
  );
  
  -- البحث في الفهرس (لا يقرأ النصوص!)
  SET results = (
    SELECT ARRAY_AGG(STRUCT(
      base.passage_id,
      distance
    ) ORDER BY distance LIMIT top_k)
    FROM VECTOR_SEARCH(
      TABLE `iqra12.semantic.passage_embeddings`,
      'embedding',
      (SELECT query_embedding AS embedding),
      top_k => top_k * 2,  -- نجلب أكثر للفلترة
      distance_type => 'COSINE'
    )
  );
END;

-- المرحلة 2: جلب النصوص (عند الحاجة)
CREATE OR REPLACE PROCEDURE `iqra12.search.fetch_passages`(
  passage_ids ARRAY<STRING>,
  OUT passages ARRAY<STRUCT<
    passage_id STRING,
    text STRING,
    work_title STRING,
    author_name STRING
  >>
)
BEGIN
  SET passages = (
    SELECT ARRAY_AGG(STRUCT(
      p.passage_id,
      p.text,
      w.title AS work_title,
      s.name AS author_name
    ))
    FROM `iqra12.curated.passages` p
    JOIN `iqra12.curated.works` w ON p.work_id = w.work_id
    JOIN `iqra12.curated.scholars` s ON w.author_id = s.scholar_id
    WHERE p.passage_id IN UNNEST(passage_ids)
  );
END;

-- المرحلة 3: التوسع للسياق (اختياري)
CREATE OR REPLACE PROCEDURE `iqra12.search.expand_context`(
  passage_id STRING,
  context_size INT64,  -- عدد المقاطع قبل وبعد
  OUT context ARRAY<STRUCT<
    passage_id STRING,
    text STRING,
    position STRING  -- 'before', 'target', 'after'
  >>
)
BEGIN
  DECLARE target_work_id STRING;
  DECLARE target_order INT64;
  
  -- جلب معلومات المقطع المستهدف
  SET (target_work_id, target_order) = (
    SELECT AS STRUCT work_id, passage_order
    FROM `iqra12.curated.passages`
    WHERE passage_id = passage_id
  );
  
  -- جلب السياق
  SET context = (
    SELECT ARRAY_AGG(STRUCT(
      passage_id,
      text,
      CASE 
        WHEN passage_order < target_order THEN 'before'
        WHEN passage_order = target_order THEN 'target'
        ELSE 'after'
      END AS position
    ) ORDER BY passage_order)
    FROM `iqra12.curated.passages`
    WHERE work_id = target_work_id
      AND passage_order BETWEEN target_order - context_size 
                            AND target_order + context_size
  );
END;
```

---

# 15. نماذج الذكاء: الخيارات والتوزيع

## 15.1 خريطة النماذج

```
┌─────────────────────────────────────────────────────────────┐
│                  خريطة النماذج في إقرأ-12                   │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                  طبقة التوليد                       │   │
│  │  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐   │   │
│  │  │ Claude  │ │ GPT-4   │ │ Gemini  │ │ Jais    │   │   │
│  │  │ Opus    │ │ Turbo   │ │ Pro     │ │ 30B     │   │   │
│  │  └────┬────┘ └────┬────┘ └────┬────┘ └────┬────┘   │   │
│  │       │           │           │           │         │   │
│  │       └───────────┴─────┬─────┴───────────┘         │   │
│  │                         │                           │   │
│  │                    [Router]                         │   │
│  │                         │                           │   │
│  │  المهمة ──────────────►│◄─────────── النموذج       │   │
│  │  كتابة أكاديمية        │          Claude Opus      │   │
│  │  حوار سقراطي           │          Gemini Pro       │   │
│  │  سياق إسلامي           │          Jais 30B         │   │
│  │  تصنيف بسيط            │          DeepSeek/Qwen    │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                  طبقة Embedding                     │   │
│  │  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐   │   │
│  │  │ text-multi- │ │ Gemini      │ │ e5-small    │   │   │
│  │  │ lingual-002 │ │ Embedding   │ │ (self-host) │   │   │
│  │  └──────┬──────┘ └──────┬──────┘ └──────┬──────┘   │   │
│  │         │               │               │           │   │
│  │         └───────────────┼───────────────┘           │   │
│  │                         │                           │   │
│  │  الاستخدام ────────────►│◄─────────── النموذج       │   │
│  │  فهرسة ضخمة (batch)     │          e5-small        │   │
│  │  بحث دلالي (online)     │          text-multi-002  │   │
│  │  cross-modal             │          Gemini Embed    │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                  طبقة المهام البسيطة               │   │
│  │  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐   │   │
│  │  │ DeepSeek    │ │ Qwen-72B    │ │ Gemini      │   │   │
│  │  │ V3         │ │             │ │ Flash       │   │   │
│  │  └──────┬──────┘ └──────┬──────┘ └──────┬──────┘   │   │
│  │         │               │               │           │   │
│  │  المهمة: تصنيف، استخراج entities، ترجمة          │   │
│  │  التكلفة: 1/10 إلى 1/100 من النماذج الكبيرة       │   │
│  └─────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
```

## 15.2 جدول التكاليف المقارن

| النموذج | السعر (لكل 1M tokens) | العربية | الاستخدام الأمثل |
|---------|----------------------|---------|-----------------|
| **Claude Opus** | $15 input / $75 output | ممتاز | كتابة أكاديمية، تحليل معقد |
| **GPT-4 Turbo** | $10 / $30 | جيد جداً | توليد متوازن |
| **Gemini Pro** | $1.25 / $5 | ممتاز | حوار، تلخيص |
| **Gemini Flash** | $0.075 / $0.3 | ممتاز | مهام سريعة، تصنيف |
| **Jais 30B** | ~$2 (self-host) | ممتاز++ | سياق إسلامي |
| **DeepSeek V3** | $0.27 / $1.10 | جيد | تصنيف، استخراج |
| **Qwen-72B** | $0.50 / $1.50 | جيد | مهام متوسطة |
| **text-embedding-005** | $0.025 / 1M chars | جيد | فهرسة إنجليزية |
| **text-multilingual-002** | $0.025 / 1M chars | ممتاز | فهرسة عربية |
| **e5-small** | ~$0.001 (self-host) | جيد | فهرسة ضخمة |

## 15.3 كود التوجيه الذكي (Model Router)

```python
"""
موجه النماذج الذكي
Smart Model Router for IQRA-12
"""

from enum import Enum
from dataclasses import dataclass
from typing import Optional, Dict, Any
import re

class TaskType(Enum):
    # مهام عالية التعقيد (Claude/GPT-4)
    ACADEMIC_WRITING = 'academic_writing'
    COMPLEX_ANALYSIS = 'complex_analysis'
    SHARIAH_AUDIT = 'shariah_audit'
    
    # مهام متوسطة (Gemini Pro/Jais)
    SUMMARIZATION = 'summarization'
    DIALOGUE = 'dialogue'
    ISLAMIC_CONTEXT = 'islamic_context'
    
    # مهام بسيطة (DeepSeek/Qwen/Flash)
    CLASSIFICATION = 'classification'
    ENTITY_EXTRACTION = 'entity_extraction'
    TRANSLATION = 'translation'
    SIMPLE_QA = 'simple_qa'
    
    # Embedding
    EMBEDDING_BATCH = 'embedding_batch'
    EMBEDDING_ONLINE = 'embedding_online'

class ModelTier(Enum):
    PREMIUM = 'premium'      # Claude Opus, GPT-4
    STANDARD = 'standard'    # Gemini Pro, Jais
    ECONOMY = 'economy'      # DeepSeek, Qwen, Flash
    EMBEDDING = 'embedding'

@dataclass
class ModelConfig:
    name: str
    tier: ModelTier
    cost_per_1k_input: float
    cost_per_1k_output: float
    arabic_score: float      # 0-1
    islamic_score: float     # 0-1
    speed_score: float       # 0-1
    max_context: int

MODELS = {
    'claude-opus': ModelConfig(
        name='claude-3-opus-20240229',
        tier=ModelTier.PREMIUM,
        cost_per_1k_input=0.015,
        cost_per_1k_output=0.075,
        arabic_score=0.9,
        islamic_score=0.7,
        speed_score=0.6,
        max_context=200000
    ),
    'gemini-pro': ModelConfig(
        name='gemini-1.5-pro',
        tier=ModelTier.STANDARD,
        cost_per_1k_input=0.00125,
        cost_per_1k_output=0.005,
        arabic_score=0.92,
        islamic_score=0.75,
        speed_score=0.85,
        max_context=1000000
    ),
    'gemini-flash': ModelConfig(
        name='gemini-1.5-flash',
        tier=ModelTier.ECONOMY,
        cost_per_1k_input=0.000075,
        cost_per_1k_output=0.0003,
        arabic_score=0.88,
        islamic_score=0.7,
        speed_score=0.95,
        max_context=1000000
    ),
    'jais-30b': ModelConfig(
        name='jais-30b-chat',
        tier=ModelTier.STANDARD,
        cost_per_1k_input=0.002,
        cost_per_1k_output=0.002,
        arabic_score=0.98,
        islamic_score=0.95,
        speed_score=0.7,
        max_context=8000
    ),
    'deepseek-v3': ModelConfig(
        name='deepseek-chat',
        tier=ModelTier.ECONOMY,
        cost_per_1k_input=0.00027,
        cost_per_1k_output=0.0011,
        arabic_score=0.75,
        islamic_score=0.5,
        speed_score=0.9,
        max_context=64000
    ),
}

TASK_TO_TIER = {
    TaskType.ACADEMIC_WRITING: ModelTier.PREMIUM,
    TaskType.COMPLEX_ANALYSIS: ModelTier.PREMIUM,
    TaskType.SHARIAH_AUDIT: ModelTier.STANDARD,  # Jais preferred
    TaskType.SUMMARIZATION: ModelTier.STANDARD,
    TaskType.DIALOGUE: ModelTier.STANDARD,
    TaskType.ISLAMIC_CONTEXT: ModelTier.STANDARD,  # Jais preferred
    TaskType.CLASSIFICATION: ModelTier.ECONOMY,
    TaskType.ENTITY_EXTRACTION: ModelTier.ECONOMY,
    TaskType.TRANSLATION: ModelTier.ECONOMY,
    TaskType.SIMPLE_QA: ModelTier.ECONOMY,
}

class ModelRouter:
    """
    موجه النماذج: يختار النموذج الأمثل لكل مهمة
    
    المبدأ: "النموذج الأغلى للمهمة الأصعب فقط"
    """
    
    def __init__(self, budget_mode: str = 'balanced'):
        """
        budget_mode:
        - 'economy': دائماً الأرخص
        - 'balanced': توازن بين الجودة والتكلفة
        - 'quality': دائماً الأفضل
        """
        self.budget_mode = budget_mode
        self.usage_log = []
    
    def route(
        self, 
        task_type: TaskType,
        input_text: str,
        requirements: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        اختيار النموذج المناسب
        
        Returns:
            model_name: اسم النموذج المختار
        """
        requirements = requirements or {}
        
        # 1. تحديد الطبقة المطلوبة
        base_tier = TASK_TO_TIER.get(task_type, ModelTier.STANDARD)
        
        # 2. تعديل حسب budget_mode
        if self.budget_mode == 'economy':
            tier = ModelTier.ECONOMY
        elif self.budget_mode == 'quality':
            tier = ModelTier.PREMIUM
        else:
            tier = base_tier
        
        # 3. تعديل حسب المتطلبات الخاصة
        if requirements.get('islamic_context'):
            # نفضل Jais للسياق الإسلامي
            if self._is_islamic_content(input_text):
                return 'jais-30b'
        
        if requirements.get('arabic_critical'):
            # نفضل النماذج الأقوى بالعربية
            tier = max(tier, ModelTier.STANDARD, key=lambda t: t.value)
        
        # 4. اختيار النموذج من الطبقة
        candidates = [
            (name, cfg) for name, cfg in MODELS.items()
            if cfg.tier == tier
        ]
        
        if not candidates:
            # fallback
            return 'gemini-pro'
        
        # 5. ترتيب حسب المعايير
        def score_model(item):
            name, cfg = item
            score = 0
            
            if requirements.get('arabic_critical'):
                score += cfg.arabic_score * 2
            if requirements.get('islamic_context'):
                score += cfg.islamic_score * 2
            if requirements.get('speed_priority'):
                score += cfg.speed_score
            
            # التكلفة (عكسي)
            score -= (cfg.cost_per_1k_input + cfg.cost_per_1k_output) * 10
            
            return score
        
        best = max(candidates, key=score_model)
        selected_model = best[0]
        
        # 6. تسجيل الاستخدام
        self._log_usage(task_type, selected_model, len(input_text))
        
        return selected_model
    
    def _is_islamic_content(self, text: str) -> bool:
        """كشف المحتوى الإسلامي"""
        indicators = [
            r'قال الله', r'قال رسول الله', r'حديث', r'آية',
            r'الإمام', r'المذهب', r'الفقه', r'الشريعة',
            r'حلال', r'حرام', r'مكروه', r'مستحب',
            r'صلى الله عليه وسلم', r'رضي الله عنه'
        ]
        return any(re.search(p, text) for p in indicators)
    
    def _log_usage(self, task: TaskType, model: str, input_length: int):
        """تسجيل الاستخدام للمراقبة"""
        self.usage_log.append({
            'task': task.value,
            'model': model,
            'input_tokens': input_length // 4,  # تقدير
            'timestamp': 'now'
        })
    
    def get_cost_report(self) -> Dict:
        """تقرير التكاليف"""
        total_cost = 0
        by_model = {}
        
        for entry in self.usage_log:
            model = entry['model']
            tokens = entry['input_tokens']
            cfg = MODELS[model]
            cost = tokens * cfg.cost_per_1k_input / 1000
            
            total_cost += cost
            by_model[model] = by_model.get(model, 0) + cost
        
        return {
            'total_cost': total_cost,
            'by_model': by_model,
            'total_requests': len(self.usage_log)
        }
```

---

# 16. وكيل البنية التحتية: التصميم النهائي

## 16.1 الهيكل العام

```
┌─────────────────────────────────────────────────────────────┐
│              Infrastructure Intelligence Agent              │
│                    وكيل البنية التحتية                      │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                    Knowledge Core                    │   │
│  │                    (نواة المعرفة)                    │   │
│  │  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐   │   │
│  │  │BigQuery │ │Vertex AI│ │ Storage │ │Composer │   │   │
│  │  │  Docs   │ │  Docs   │ │  Docs   │ │  Docs   │   │   │
│  │  └─────────┘ └─────────┘ └─────────┘ └─────────┘   │   │
│  │  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐   │   │
│  │  │Pricing  │ │ Best    │ │ Common  │ │IQRA-12  │   │   │
│  │  │ Tables  │ │Practices│ │ Errors  │ │ Schemas │   │   │
│  │  └─────────┘ └─────────┘ └─────────┘ └─────────┘   │   │
│  └─────────────────────────────────────────────────────┘   │
│                           │                                 │
│                           ▼                                 │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                   Action Layer                       │   │
│  │                  (طبقة الفعل)                        │   │
│  │                                                      │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌────────────┐   │   │
│  │  │   Monitor   │  │   Advise    │  │  Execute   │   │   │
│  │  │   (رصد)     │  │  (نصح)     │  │  (تنفيذ)   │   │   │
│  │  │             │  │             │  │            │   │   │
│  │  │ • التكاليف  │  │ • التحسين   │  │ • SQL      │   │   │
│  │  │ • الأداء    │  │ • التصميم   │  │ • Jobs     │   │   │
│  │  │ • الأخطاء   │  │ • الاختيار  │  │ • Schemas  │   │   │
│  │  │ • الموارد   │  │ • التنبيه   │  │ • Pipelines│   │   │
│  │  └─────────────┘  └─────────────┘  └────────────┘   │   │
│  └─────────────────────────────────────────────────────┘   │
│                           │                                 │
│                           ▼                                 │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                  Interface Layer                     │   │
│  │                 (طبقة الواجهة)                       │   │
│  │                                                      │   │
│  │  ┌─────────────┐  ┌─────────────┐  ┌────────────┐   │   │
│  │  │   Chat      │  │  Dashboard  │  │   Alerts   │   │   │
│  │  │  (محادثة)   │  │  (لوحة)    │  │ (تنبيهات) │   │   │
│  │  └─────────────┘  └─────────────┘  └────────────┘   │   │
│  └─────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
```

## 16.2 الكود الحرج

```python
"""
وكيل البنية التحتية
Infrastructure Intelligence Agent for IQRA-12
"""

from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional
from enum import Enum
from datetime import datetime, timedelta
import json

class ActionType(Enum):
    MONITOR = 'monitor'     # مراقبة فقط
    ADVISE = 'advise'       # نصيحة بدون تنفيذ
    EXECUTE = 'execute'     # تنفيذ مباشر
    ESCALATE = 'escalate'   # تصعيد للمسؤول

class AlertSeverity(Enum):
    INFO = 'info'
    WARNING = 'warning'
    CRITICAL = 'critical'
    EMERGENCY = 'emergency'

@dataclass
class CostAlert:
    severity: AlertSeverity
    message: str
    current_cost: float
    threshold: float
    recommendation: str
    timestamp: datetime = field(default_factory=datetime.now)

@dataclass
class QueryAnalysis:
    query: str
    estimated_cost: float
    estimated_bytes: int
    optimization_suggestions: List[str]
    risk_level: str  # 'low', 'medium', 'high', 'critical'
    recommended_action: ActionType

class InfrastructureAgent:
    """
    وكيل البنية التحتية
    
    الدور: خبير GCP يعمل 24/7 لتوفير المال والوقت والجهد
    
    القدرات:
    1. المعرفة العميقة بـ BigQuery, Vertex, Storage, Composer
    2. المراقبة المستمرة للتكاليف والأداء
    3. التنبيه الذكي قبل الكوارث
    4. التنفيذ الآمن للمهام الروتينية
    """
    
    def __init__(self, bq_client, vertex_client, storage_client):
        self.bq = bq_client
        self.vertex = vertex_client
        self.storage = storage_client
        
        # الإعدادات
        self.daily_budget = 50.0  # دولار
        self.query_cost_threshold = 5.0  # دولار لكل استعلام
        self.alert_thresholds = {
            'daily_50': 0.5,   # 50% من الميزانية
            'daily_80': 0.8,   # 80%
            'daily_100': 1.0,  # 100%
        }
        
        # السجلات
        self.alerts: List[CostAlert] = []
        self.actions_log: List[Dict] = []
    
    # ═══════════════════════════════════════════════════════
    # طبقة المراقبة (Monitor)
    # ═══════════════════════════════════════════════════════
    
    def get_daily_costs(self) -> Dict[str, float]:
        """جلب تكاليف اليوم"""
        query = """
        SELECT
            DATE(creation_time) as day,
            user_email,
            SUM(total_bytes_billed) / POW(10, 12) * 5 as cost_usd,
            COUNT(*) as query_count,
            SUM(total_slot_ms) / 1000 as total_slot_seconds
        FROM `region-us`.INFORMATION_SCHEMA.JOBS_BY_PROJECT
        WHERE DATE(creation_time) = CURRENT_DATE()
        GROUP BY 1, 2
        ORDER BY cost_usd DESC
        """
        results = self.bq.query(query)
        
        total_cost = sum(r['cost_usd'] for r in results)
        
        # فحص العتبات
        self._check_cost_thresholds(total_cost)
        
        return {
            'total': total_cost,
            'by_user': {r['user_email']: r['cost_usd'] for r in results},
            'query_count': sum(r['query_count'] for r in results)
        }
    
    def _check_cost_thresholds(self, current_cost: float):
        """فحص تجاوز العتبات"""
        ratio = current_cost / self.daily_budget
        
        if ratio >= 1.0:
            self._create_alert(
                AlertSeverity.EMERGENCY,
                f"تجاوز الميزانية اليومية! ${current_cost:.2f} / ${self.daily_budget}",
                current_cost,
                self.daily_budget,
                "إيقاف الاستعلامات غير الضرورية فوراً"
            )
        elif ratio >= 0.8:
            self._create_alert(
                AlertSeverity.CRITICAL,
                f"اقتراب من الميزانية: {ratio*100:.0f}%",
                current_cost,
                self.daily_budget,
                "مراجعة الاستعلامات الجارية وتأجيل غير العاجل"
            )
        elif ratio >= 0.5:
            self._create_alert(
                AlertSeverity.WARNING,
                f"تم استهلاك {ratio*100:.0f}% من الميزانية",
                current_cost,
                self.daily_budget,
                "مراقبة الاستعلامات الكبيرة"
            )
    
    def _create_alert(self, severity, message, current, threshold, recommendation):
        """إنشاء تنبيه"""
        alert = CostAlert(
            severity=severity,
            message=message,
            current_cost=current,
            threshold=threshold,
            recommendation=recommendation
        )
        self.alerts.append(alert)
        
        # إرسال التنبيه (في الإنتاج: Slack, Email, etc.)
        self._send_alert(alert)
    
    def get_expensive_queries(self, days: int = 7, top_n: int = 10) -> List[Dict]:
        """جلب أغلى الاستعلامات"""
        query = f"""
        SELECT
            job_id,
            user_email,
            query,
            total_bytes_billed / POW(10, 12) * 5 as cost_usd,
            total_slot_ms / 1000 as slot_seconds,
            creation_time
        FROM `region-us`.INFORMATION_SCHEMA.JOBS_BY_PROJECT
        WHERE DATE(creation_time) >= DATE_SUB(CURRENT_DATE(), INTERVAL {days} DAY)
            AND job_type = 'QUERY'
            AND total_bytes_billed > 0
        ORDER BY cost_usd DESC
        LIMIT {top_n}
        """
        return list(self.bq.query(query))
    
    # ═══════════════════════════════════════════════════════
    # طبقة النصح (Advise)
    # ═══════════════════════════════════════════════════════
    
    def analyze_query(self, query: str) -> QueryAnalysis:
        """تحليل استعلام قبل تنفيذه"""
        # Dry run للتقدير
        job_config = self.bq.QueryJobConfig(dry_run=True)
        job = self.bq.query(query, job_config=job_config)
        
        estimated_bytes = job.total_bytes_processed
        estimated_cost = (estimated_bytes / 1e12) * 5  # $5 per TB
        
        # تحليل الاستعلام
        suggestions = self._get_optimization_suggestions(query, estimated_bytes)
        risk_level = self._assess_query_risk(estimated_cost)
        
        # تحديد الإجراء المناسب
        if estimated_cost > self.query_cost_threshold:
            action = ActionType.ESCALATE
        elif estimated_cost > 1.0:
            action = ActionType.ADVISE
        else:
            action = ActionType.EXECUTE
        
        return QueryAnalysis(
            query=query,
            estimated_cost=estimated_cost,
            estimated_bytes=estimated_bytes,
            optimization_suggestions=suggestions,
            risk_level=risk_level,
            recommended_action=action
        )
    
    def _get_optimization_suggestions(self, query: str, bytes_est: int) -> List[str]:
        """اقتراحات تحسين الاستعلام"""
        suggestions = []
        query_upper = query.upper()
        
        # فحص SELECT *
        if 'SELECT *' in query_upper:
            suggestions.append("⚠️ تجنب SELECT * - حدد الأعمدة المطلوبة فقط")
        
        # فحص غياب WHERE
        if 'WHERE' not in query_upper and bytes_est > 1e9:  # > 1GB
            suggestions.append("⚠️ أضف شرط WHERE لتقليل البيانات المقروءة")
        
        # فحص Partition
        if 'PARTITION' not in query_upper and bytes_est > 10e9:  # > 10GB
            suggestions.append("💡 استخدم فلتر على عمود التقسيم")
        
        # فحص LIMIT بدون ORDER BY
        if 'LIMIT' in query_upper and 'ORDER BY' not in query_upper:
            suggestions.append("💡 النتائج بدون ORDER BY قد تكون غير متسقة")
        
        # فحص JOINs
        join_count = query_upper.count('JOIN')
        if join_count > 3:
            suggestions.append(f"⚠️ {join_count} JOINs - فكر في Denormalization")
        
        return suggestions
    
    def _assess_query_risk(self, cost: float) -> str:
        """تقييم خطورة الاستعلام"""
        if cost > 10:
            return 'critical'
        elif cost > 5:
            return 'high'
        elif cost > 1:
            return 'medium'
        else:
            return 'low'
    
    def recommend_table_design(self, table_spec: Dict) -> Dict:
        """توصيات تصميم الجدول"""
        recommendations = {
            'partition': None,
            'cluster': None,
            'denormalization': [],
            'warnings': []
        }
        
        estimated_size = table_spec.get('estimated_size_gb', 0)
        columns = table_spec.get('columns', [])
        common_queries = table_spec.get('common_queries', [])
        
        # التقسيم
        if estimated_size > 1:
            date_cols = [c for c in columns if 'date' in c.lower() or 'time' in c.lower()]
            if date_cols:
                recommendations['partition'] = {
                    'type': 'TIME',
                    'column': date_cols[0],
                    'reason': 'وجود عمود زمني + حجم كبير'
                }
            else:
                recommendations['partition'] = {
                    'type': 'RANGE',
                    'suggestion': 'أضف عمود زمني أو استخدم INTEGER range',
                    'reason': 'الحجم الكبير يستدعي التقسيم'
                }
        
        # التجميع
        if common_queries:
            filter_cols = self._extract_filter_columns(common_queries)
            if filter_cols:
                recommendations['cluster'] = {
                    'columns': filter_cols[:4],  # حد 4 أعمدة
                    'reason': 'أكثر الأعمدة استخداماً في الفلترة'
                }
        
        return recommendations
    
    # ═══════════════════════════════════════════════════════
    # طبقة التنفيذ (Execute)
    # ═══════════════════════════════════════════════════════
    
    def execute_safe_query(self, query: str, max_cost: float = 1.0) -> Any:
        """تنفيذ استعلام مع حماية التكلفة"""
        # تحليل أولاً
        analysis = self.analyze_query(query)
        
        if analysis.estimated_cost > max_cost:
            raise ValueError(
                f"تكلفة الاستعلام (${analysis.estimated_cost:.2f}) "
                f"تتجاوز الحد المسموح (${max_cost:.2f}). "
                f"الاقتراحات: {analysis.optimization_suggestions}"
            )
        
        # تنفيذ
        result = self.bq.query(query)
        
        # تسجيل
        self._log_action('execute_query', {
            'query': query[:200],
            'cost': analysis.estimated_cost,
            'rows': result.total_rows
        })
        
        return result
    
    def create_optimized_table(
        self, 
        dataset: str, 
        table_name: str,
        schema: List[Dict],
        partition_column: Optional[str] = None,
        cluster_columns: Optional[List[str]] = None
    ) -> str:
        """إنشاء جدول محسّن"""
        # بناء DDL
        ddl_parts = [f"CREATE TABLE `{dataset}.{table_name}` ("]
        
        # الأعمدة
        col_defs = []
        for col in schema:
            col_def = f"  {col['name']} {col['type']}"
            if col.get('not_null'):
                col_def += ' NOT NULL'
            if col.get('description'):
                col_def += f" OPTIONS(description='{col['description']}')"
            col_defs.append(col_def)
        ddl_parts.append(',\n'.join(col_defs))
        ddl_parts.append(')')
        
        # التقسيم
        if partition_column:
            ddl_parts.append(f"PARTITION BY DATE({partition_column})")
        
        # التجميع
        if cluster_columns:
            ddl_parts.append(f"CLUSTER BY {', '.join(cluster_columns)}")
        
        ddl = '\n'.join(ddl_parts)
        
        # تنفيذ
        self.bq.query(ddl)
        
        self._log_action('create_table', {
            'table': f"{dataset}.{table_name}",
            'partition': partition_column,
            'cluster': cluster_columns
        })
        
        return ddl
    
    def schedule_batch_job(
        self,
        name: str,
        query: str,
        schedule: str,  # cron expression
        destination_table: str
    ) -> str:
        """جدولة مهمة دفعية"""
        # في الإنتاج: استخدام BigQuery Scheduled Queries أو Composer
        job_config = {
            'name': name,
            'query': query,
            'schedule': schedule,
            'destination': destination_table,
            'write_disposition': 'WRITE_TRUNCATE'
        }
        
        # إنشاء الجدولة (pseudo-code)
        # self.bq.create_scheduled_query(job_config)
        
        self._log_action('schedule_job', job_config)
        
        return f"Scheduled: {name} @ {schedule}"
    
    # ═══════════════════════════════════════════════════════
    # المساعدات
    # ═══════════════════════════════════════════════════════
    
    def _log_action(self, action_type: str, details: Dict):
        """تسجيل الإجراء"""
        self.actions_log.append({
            'type': action_type,
            'details': details,
            'timestamp': datetime.now().isoformat()
        })
    
    def _send_alert(self, alert: CostAlert):
        """إرسال التنبيه (في الإنتاج: Slack, Email)"""
        print(f"[{alert.severity.value.upper()}] {alert.message}")
        print(f"  التوصية: {alert.recommendation}")
    
    def _extract_filter_columns(self, queries: List[str]) -> List[str]:
        """استخراج الأعمدة الأكثر استخداماً في WHERE"""
        # تحليل مبسط
        import re
        columns = []
        for q in queries:
            matches = re.findall(r'WHERE\s+(\w+)\s*=', q, re.IGNORECASE)
            columns.extend(matches)
        
        # ترتيب بالتكرار
        from collections import Counter
        return [c for c, _ in Counter(columns).most_common()]
    
    def get_status_report(self) -> Dict:
        """تقرير الحالة الشامل"""
        daily_costs = self.get_daily_costs()
        
        return {
            'timestamp': datetime.now().isoformat(),
            'costs': {
                'today': daily_costs['total'],
                'budget': self.daily_budget,
                'utilization': daily_costs['total'] / self.daily_budget
            },
            'alerts': [
                {
                    'severity': a.severity.value,
                    'message': a.message
                } for a in self.alerts[-10:]  # آخر 10
            ],
            'actions': len(self.actions_log),
            'recommendations': self._generate_recommendations()
        }
    
    def _generate_recommendations(self) -> List[str]:
        """توليد توصيات عامة"""
        recs = []
        
        # فحص الاستعلامات المكلفة
        expensive = self.get_expensive_queries(days=1, top_n=5)
        if expensive:
            total_expensive = sum(q['cost_usd'] for q in expensive)
            if total_expensive > self.daily_budget * 0.5:
                recs.append(
                    f"💰 أغلى 5 استعلامات اليوم = ${total_expensive:.2f}. "
                    "راجع إمكانية تحسينها."
                )
        
        return recs
```

---

# 17. الخلاصة التنفيذية

## 17.1 ملخص المبادئ

| المبدأ | التطبيق | الأداة |
|--------|---------|--------|
| **التخزين رخيص، القراءة غالية** | SELECT محدد، Partition، Cluster | BigQuery DDL |
| **لا تخزن المكرر** | كشف التناص، تخزين الدلتا | IntertextualityDetector |
| **النموذج الأغلى للأصعب** | توجيه المهام حسب التعقيد | ModelRouter |
| **Batch > Streaming** | تأجيل ما يمكن تأجيله | Composer/Workflows |
| **المراقبة قبل الكارثة** | تنبيهات الميزانية، Dry Run | InfrastructureAgent |

## 17.2 الخطوة التالية

**بناء الوثيقة الرسمية Word؟** أم **الانتقال لتسلسل العمل الأكاديمي والإداري؟**
