"""
Full Heritage Text Processor
Ù…Ø¹Ø§Ù„Ø¬Ø© 157M Ù†Øµ - ÙŠØ¹Ù…Ù„ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
"""

import asyncio
import json
from google.cloud import bigquery
from transformers import AutoTokenizer, AutoModel
import torch
from datetime import datetime
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/home/user/processing.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)


class FullProcessor:
    """Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„ÙƒØ§Ù…Ù„"""
    
    def __init__(self):
        self.bq_client = bigquery.Client(project="iqraa-12")
        
        # ØªØ­Ù…ÙŠÙ„ CAMeLBERT
        logger.info("ğŸ“¥ ØªØ­Ù…ÙŠÙ„ CAMeLBERT-CA...")
        self.tokenizer = AutoTokenizer.from_pretrained("CAMeL-Lab/bert-base-arabic-camelbert-ca")
        self.model = AutoModel.from_pretrained("CAMeL-Lab/bert-base-arabic-camelbert-ca")
        self.model.eval()
        
        # ØªØªØ¨Ø¹
        self.tracker = {
            "started_at": datetime.now().isoformat(),
            "processed": 0,
            "cost": 0,
            "errors": 0
        }
        
        logger.info("âœ… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø¬Ø§Ù‡Ø²")
    
    def save_checkpoint(self):
        """Ø­ÙØ¸ Ù†Ù‚Ø·Ø© Ø§Ø³ØªØ¹Ø§Ø¯Ø©"""
        with open("/home/user/processing_checkpoint.json", "w") as f:
            json.dump(self.tracker, f, indent=2)
    
    async def process_batch(self, batch_size=1000):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙØ¹Ø©"""
        
        # Ø¬Ù„Ø¨ Ø¯ÙØ¹Ø© Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø®Ø§Ù…
        query = f"""
        SELECT chunk_id, record_id, text
        FROM `iqraa-12.diwan_iqraa_v2.openiti_chunks`
        WHERE chunk_id NOT IN (
            SELECT chunk_id FROM `iqraa-12.diwan_iqraa_v2.classifications_unified`
        )
        LIMIT {batch_size}
        """
        
        rows = list(self.bq_client.query(query).result())
        
        if not rows:
            logger.info("âœ… ÙƒÙ„ Ø§Ù„Ù†ØµÙˆØµ Ù…ÙØ¹Ø§Ù„Ø¬Ø©!")
            return False
        
        logger.info(f"ğŸ”„ Ù…Ø¹Ø§Ù„Ø¬Ø© {len(rows)} Ù†Øµ...")
        
        results = []
        for row in rows:
            try:
                # Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ù€ CAMeLBERT
                inputs = self.tokenizer(row.text[:512], return_tensors="pt", truncation=True)
                
                with torch.no_grad():
                    outputs = self.model(**inputs)
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ Ù…Ù† embeddings
                # (Ø³Ù†ÙØ¶ÙŠÙ classifier head Ù„Ø§Ø­Ù‚Ø§Ù‹)
                
                result = {
                    "chunk_id": row.chunk_id,
                    "record_id": row.record_id,
                    "text": row.text,
                    "camelbert_processed": True,
                    "processed_at": datetime.now().isoformat()
                }
                
                results.append(result)
                self.tracker["processed"] += 1
                
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ {row.chunk_id}: {e}")
                self.tracker["errors"] += 1
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        if results:
            errors = self.bq_client.insert_rows_json(
                "iqraa-12.diwan_iqraa_v2.classifications_unified",
                results
            )
            
            if errors:
                logger.error(f"âŒ Ø£Ø®Ø·Ø§Ø¡ ÙÙŠ Ø§Ù„Ø­ÙØ¸: {errors[:3]}")
        
        # Ø­ÙØ¸ checkpoint
        if self.tracker["processed"] % 10000 == 0:
            self.save_checkpoint()
            logger.info(f"ğŸ’¾ Checkpoint: {self.tracker['processed']:,} Ù†Øµ")
        
        return True
    
    async def run(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©"""
        
        logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©...")
        logger.info(f"   Ø§Ù„Ù‡Ø¯Ù: 157,870,756 Ù†Øµ")
        logger.info(f"   Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©: $12,895")
        
        while True:
            has_more = await self.process_batch(1000)
            
            if not has_more:
                break
            
            # Ø¹Ø±Ø¶ Ø§Ù„ØªÙ‚Ø¯Ù…
            if self.tracker["processed"] % 100000 == 0:
                percentage = self.tracker["processed"] / 157870756 * 100
                logger.info(f"ğŸ“Š Ø§Ù„ØªÙ‚Ø¯Ù…: {self.tracker['processed']:,} ({percentage:.2f}%)")
        
        logger.info("âœ… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§ÙƒØªÙ…Ù„Øª!")
        self.save_checkpoint()


# ØªØ´ØºÙŠÙ„
if __name__ == "__main__":
    processor = FullProcessor()
    asyncio.run(processor.run())

