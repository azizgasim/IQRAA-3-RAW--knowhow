#!/usr/bin/env python3
"""
Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù†ØµÙˆØµ
ÙŠØµÙ†Ù Ø§Ù„Ù†ØµÙˆØµ Ø¹Ù„Ù‰ 15 ÙØ¦Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
"""
import os
import json
import re
from pathlib import Path
from typing import Dict, List, Tuple
from collections import defaultdict
from datetime import datetime

# Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
CONFIG_PATH = Path("./pipeline/config/tables_schema.json")
INPUT_PATH = Path(os.getenv("DATA_PROCESSED_PATH", "./data/processed/chunked"))
OUTPUT_PATH = Path("./data/processed/classified")

def load_categories() -> Dict:
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙØ¦Ø§Øª ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
    if CONFIG_PATH.exists():
        with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
            schema = json.load(f)
        return schema.get("categories", {})
    return {}

def classify_text(text: str, metadata: Dict, categories: Dict) -> List[Tuple[str, float]]:
    """ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Øµ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"""
    scores = defaultdict(float)
    text_lower = text.lower()
    
    title = metadata.get("book_id", "").lower()
    author = metadata.get("author_id", "").lower()
    
    for cat_id, cat_info in categories.items():
        keywords = cat_info.get("keywords", [])
        
        for keyword in keywords:
            count = len(re.findall(keyword, text_lower))
            scores[cat_id] += count * 1.0
            
            if keyword in title:
                scores[cat_id] += 5.0
            
            if keyword in author:
                scores[cat_id] += 3.0
    
    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    
    max_score = sorted_scores[0][1] if sorted_scores and sorted_scores[0][1] > 0 else 1
    normalized = [(cat, score/max_score) for cat, score in sorted_scores if score > 0]
    
    return normalized[:3]

def classify_chunk(chunk: Dict, metadata: Dict, categories: Dict) -> Dict:
    """ØªØµÙ†ÙŠÙ Ø¬Ø²Ø¡ ÙˆØ§Ø­Ø¯"""
    classifications = classify_text(chunk["text"], metadata, categories)
    
    return {
        **chunk,
        "classifications": [
            {"category": cat, "confidence": round(conf, 3)}
            for cat, conf in classifications
        ],
        "primary_category": classifications[0][0] if classifications else "15_indexes"
    }

def process_chunked_file(file_path: Path, categories: Dict, output_dir: Path) -> Dict:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù Ù…Ù‚Ø·Ø¹"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    metadata = data.get("metadata", {})
    chunks = data.get("chunks", [])
    
    classified_chunks = [
        classify_chunk(chunk, metadata, categories)
        for chunk in chunks
    ]
    
    category_counts = defaultdict(int)
    for chunk in classified_chunks:
        category_counts[chunk["primary_category"]] += 1
    
    primary_category = max(category_counts, key=category_counts.get) if category_counts else "15_indexes"
    
    result = {
        **data,
        "chunks": classified_chunks,
        "file_classification": {
            "primary_category": primary_category,
            "category_distribution": dict(category_counts)
        }
    }
    
    category_dir = output_dir / primary_category
    category_dir.mkdir(parents=True, exist_ok=True)
    
    output_file = category_dir / file_path.name
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    return {
        "file": str(file_path),
        "category": primary_category,
        "chunks": len(classified_chunks)
    }

def classify_all(input_dir: Path = INPUT_PATH, output_dir: Path = OUTPUT_PATH):
    """ØªØµÙ†ÙŠÙ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª"""
    categories = load_categories()
    output_dir.mkdir(parents=True, exist_ok=True)
    
    files = list(input_dir.glob("*_chunks.json"))
    print(f"ğŸ“‚ ÙˆÙØ¬Ø¯ {len(files)} Ù…Ù„Ù Ù„Ù„ØªØµÙ†ÙŠÙ")
    
    results = []
    category_stats = defaultdict(int)
    
    for i, file_path in enumerate(files, 1):
        try:
            result = process_chunked_file(file_path, categories, output_dir)
            results.append(result)
            category_stats[result["category"]] += 1
            print(f"[{i}/{len(files)}] âœ… {file_path.name} â†’ {result['category']}")
        except Exception as e:
            print(f"[{i}/{len(files)}] âŒ {file_path.name}: {e}")
    
    report = {
        "classified_at": datetime.now().isoformat(),
        "total_files": len(files),
        "classified": len(results),
        "category_distribution": dict(category_stats)
    }
    
    with open(output_dir / "classification_report.json", 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“Š ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª:")
    for cat, count in sorted(category_stats.items()):
        print(f"  {cat}: {count} Ù…Ù„Ù")
    
    return report

if __name__ == "__main__":
    classify_all()
